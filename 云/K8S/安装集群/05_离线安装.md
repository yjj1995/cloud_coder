iso 上的dev tar 

版本: 1.28

## 前提是完成了基本的配置。

```



1. 免密
ssh-keygen -t rsa
ssh-copy-id -i .ssh/id_rsa.pub root@ip


2. 安装containerd
# 直接解压到目标目录
$ tar -zxvf containerd-1.7.2-linux-amd64.tar.gz -C /usr/local
# 解压获取service文件
$ tar -zxvf cri-containerd-1.7.2-linux-amd64.tar.gz
# 拷贝解压出来的service文件到linux系统服务
$ mv etc/systemd/system/containerd.service /etc/systemd/system/containerd.service

sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

cd /k8s_source

3. 导入tar包

sudo ctr -n k8s.io images import  kube-1.29.0.tar
 sudo ctr -n k8s.io images import calico-3.27.tar
 sudo ctr -n k8s.io images import pause.3.8.tar
 sudo ctr -n k8s.io images import pause-3.9.tar



4. 安装deb
sudo dpkg -i conntrack_1%3a1.4.6-2build2_amd64.deb cri-tools_1.29.0-1.1_amd64.deb ebtables_2.0.11-4build2_amd64.deb ethtool_1%3a5.16-1_amd64.deb kubeadm_1.29.0-1.1_amd64.deb kubectl_1.29.0-1.1_amd64.deb kubelet_1.29.0-1.1_amd64.deb kubernetes-cni_1.3.0-1.1_amd64.deb socat_1.7.4.1-3ubuntu4_amd64.deb

4.1 作用
conntrack：跟踪网络连接状态。
ethtool：配置和查询以太网设备。
kubelet：管理 Pod 和容器的生命周期。
cri-tools：与容器运行时交互。
kubeadm：简化 Kubernetes 集群的部署和管理。
kubernetes-cni：配置和管理容器网络。
ebtables：管理桥接设备上的网络规则。
kubectl：管理 Kubernetes 集群资源。
socat：建立双向数据传输连接。



5. 初始化
 sudo kubeadm init \
  --apiserver-advertise-address 10.128.118.241 \
  --kubernetes-version v1.29.0 \
  --pod-network-cidr=192.168.0.0/16 \
  --cri-socket=unix:///var/run/containerd/containerd.sock \
  --ignore-preflight-errors=all
  
6. 配置calico
# 离线
#sudo kubectl create -f installTool/k8s-plugin/calico/tigera-operator.yaml
#sudo kubectl create -f installTool/k8s-plugin/calico/custom-resources.yaml
# 在线
wget https://raw.githubusercontent.com/projectcalico/calico/v3.25.1/manifests/tigera-operator.yaml
wget https://raw.githubusercontent.com/projectcalico/calico/v3.25.1/manifests/custom-resources.yaml
vim custom-resources.yaml
......
 11     ipPools:
 12     - blockSize: 26
 13       cidr: 10.244.0.0/16  # --pod-network-cidr对应的IP地址段
 14       encapsulation: VXLANCrossSubnet
......

kubectl  create -f tigera-operator.yaml
kubectl create -f custom-resources.yaml
sudo kubectl taint nodes --all node-role.kubernetes.io/control-plane-
sudo kubectl taint nodes --all node-role.kubernetes.io/master-
sudo kubectl get nodes -o wide
```

